import streamlit as st
import json
import pandas as pd
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.font_manager as fm
from collections import Counter
import re
import numpy as np
from datetime import datetime
import os
import gc # å¼•å…¥åƒåœ¾å›æ”¶æœºåˆ¶

# ==========================================
# 0. åŸºç¡€é…ç½® & CSS
# ==========================================
st.set_page_config(page_title="ChatGPT æ·±åº¦åˆ†æ 28.0 (è½»é‡ç‰ˆ)", layout="wide", page_icon="ğŸ“Š")

st.markdown("""
<style>
section[data-testid="stSidebar"] .stMarkdown h1,
section[data-testid="stSidebar"] .stMarkdown h2,
section[data-testid="stSidebar"] .stMarkdown h3,
section[data-testid="stSidebar"] .stMarkdown h4,
section[data-testid="stSidebar"] .stMarkdown p,
section[data-testid="stSidebar"] label, 
section[data-testid="stSidebar"] .stCaption,
section[data-testid="stSidebar"] div[data-testid="stText"],
section[data-testid="stSidebar"] div[class*="stSlider"] > label,
section[data-testid="stSidebar"] div[class*="stSelectbox"] > label,
section[data-testid="stSidebar"] div[data-testid="stTextArea"] > label {
    text-align: center !important;
    width: 100% !important;
    display: block !important;
}
div[data-testid="stColorPicker"] {
    display: flex;
    justify-content: center;
    align-items: center;
    flex-direction: column;
    width: 100%;
}
section[data-testid="stSidebar"] div[data-testid="stCheckbox"] {
    display: flex;
    justify-content: center !important;
    align-items: center !important;
    width: 100% !important;
}
section[data-testid="stSidebar"] div[data-testid="stCheckbox"] > div {
    justify-content: center !important;
}
section[data-testid="stSidebar"] div[data-testid="stCheckbox"] label {
    text-align: center !important;
    width: auto !important;
}
section[data-testid="stSidebar"] > div:first-child {
    padding-top: 2rem;
}
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. å­—ä½“åŠ è½½å™¨
# ==========================================
def get_custom_font_path():
    font_path = "msyh.ttc"
    if os.path.exists(font_path):
        return font_path
    else:
        return "arial.ttf" 

def get_custom_font_prop(size=14, weight='normal'):
    fp = get_custom_font_path()
    prop = fm.FontProperties(fname=fp)
    prop.set_size(size)
    prop.set_weight(weight)
    return prop

# ==========================================
# 2. å†…ç½®åœç”¨è¯è¡¨
# ==========================================
DEFAULT_STOPWORDS = {
    "---", "...", "###", "___", "***", "=>", "->", "<-", "<=", ">=", "**", "__", "~~", "==", "!=", "&&", "||", "```", "`", ">", "|", "[", "]", "{", "}", "(", ")",
    "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£", "å—", "å§", "å•Š", "å¯ä»¥", "è¿™ä¸ª", "é‚£ä¸ª", "å…¶å®", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "ä½†æ˜¯", 
    "å—¯", "å“¦", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸º", "ä¹‹", "ä¸", "åŠ", "å…¶", "ä»¥", "å¯¹", "è€Œ", "ç€", "ä¸‹", "åœ°", "å­", "ä¸­", "å¥¹", "ä»–", "ä»¬", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å®ƒä»¬", 
    "å“ˆå“ˆ", "å“ˆå“ˆå“ˆ", "å˜¿å˜¿", "å‘ƒ", "å˜›", "å‘€", "å‘¢", "å•¦", "å“‡", "å”‰", "å“", "å“¼", "å™¢", "å‘—",
    "å“ªæ€•", "å¯èƒ½", "è§‰å¾—", "åº”è¯¥", "è¿™ç§", "é‚£ç§", "æ¯”å¦‚", "æˆ–è€…", "ä»¥åŠ", "ç„¶å", "è™½ç„¶", "ä½†æ˜¯", "ä¸è¿‡", "åªæ˜¯", "è¿™æ ·", "é‚£æ ·", "è¿™é‡Œ", "é‚£é‡Œ",
    "ç¡®å®", "çœŸçš„", "éå¸¸", "ç‰¹åˆ«", "æ¯”è¾ƒ", "ç¨å¾®", "åªè¦", "åªæœ‰", "é™¤é", "å‡ ä¹", "ç®€ç›´", "ä¼¼ä¹", "å¥½åƒ", "å¤§æ¦‚", "ä¹Ÿè®¸",
    "1", "2", "3", "4", "5", "6", "7", "8", "9", "0", ".", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "â€œ", "â€", "ï¼š", "ï¼›", "ï¼ˆ", "ï¼‰", "ã€Š", "ã€‹", "â€¦", "â€”", "ï½",
    "the", "a", "an", "and", "or", "but", "if", "because", "as", "what", "when", "where", "how", "why", "which", "who", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do", "does", "did", "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "my", "your", "his", "its", "our", "their", "this", "that", "these", "those", "there", "here", "in", "on", "at", "to", "for", "of", "with", "by", "from", "up", "down", "out", "about", "into", "over", "after", "can", "could", "will", "would", "should", "may", "might", "must", "not", "no", "yes", "ok", "okay", "well", "just", "only", "very", "really", "too", "so", "much", "many", "some", "any", "all"
}

# ==========================================
# 3. æ ¸å¿ƒè§£æå‡½æ•° (å†…å­˜ä¼˜åŒ–ç‰ˆ)
# ==========================================
@st.cache_data
def parse_and_count(file, stop_words):
    """
    ç›´æ¥åœ¨è§£æå¾ªç¯ä¸­ç»Ÿè®¡è¯é¢‘ï¼Œä¸ä¿å­˜æµ·é‡æ–‡æœ¬åˆ°å†…å­˜ã€‚
    """
    try:
        data = json.load(file)
    except:
        st.error("æ–‡ä»¶æ ¼å¼ä¸å¯¹ï¼Œè¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯ JSON æ–‡ä»¶")
        return None, None, None, None, 0, 0, 0, 0, 0, 0

    user_counter = Counter()
    ai_counter = Counter()
    
    user_timeline = [] # åªå­˜ (date, text_preview) æˆ– (date, top_keywords)
    
    u_count = 0
    u_total_len = 0
    u_max_len = 0
    
    a_count = 0
    a_total_len = 0
    a_max_len = 0

    for conversation in data:
        mapping = conversation.get('mapping', {})
        create_time = conversation.get('create_time')
        base_dt = datetime.fromtimestamp(create_time) if create_time else None
        
        for node_id, node_data in mapping.items():
            message = node_data.get('message')
            if message and message.get('content') and message.get('author'):
                role = message['author']['role']
                content_parts = message['content'].get('parts', [])
                text_content = "".join([part for part in content_parts if isinstance(part, str)])
                
                if text_content:
                    # 1. åŸºç¡€ç»Ÿè®¡
                    text_len = len(text_content)
                    
                    if role == 'user':
                        u_count += 1
                        u_total_len += text_len
                        if text_len > u_max_len: u_max_len = text_len
                        
                        # 2. è¯é¢‘ç»Ÿè®¡ (æµå¼å¤„ç†ï¼Œä¸å­˜åˆ—è¡¨)
                        words = jieba.cut(text_content) # ä½¿ç”¨ cut ç”Ÿæˆå™¨ï¼Œçœå†…å­˜
                        filtered = [w for w in words if len(w.strip()) > 1 and w.strip().lower() not in stop_words]
                        user_counter.update(filtered)
                        
                        # 3. æ—¶é—´çº¿æ•°æ® (åªå­˜å¿…è¦ä¿¡æ¯)
                        msg_time = message.get('create_time')
                        dt = datetime.fromtimestamp(msg_time) if msg_time else base_dt
                        if dt:
                            # ä¸ºäº†çœå†…å­˜ï¼Œæ—¶é—´çº¿æˆ‘ä»¬åªå­˜ "æ—¥æœŸ" å’Œ "è¿™å¥é‡Œçš„å…³é”®è¯"
                            # æˆ–è€…ç®€å•ç‚¹ï¼Œå…ˆå­˜ä¸‹æ¥ï¼Œåé¢ DataFrame å¤„ç†æ—¶å†ä¼˜åŒ–
                            user_timeline.append({"time": dt, "keywords": filtered}) # å­˜è¿‡æ»¤åçš„è¯åˆ—è¡¨æ¯”åŸæ–‡å°

                    elif role == 'assistant':
                        a_count += 1
                        a_total_len += text_len
                        if text_len > a_max_len: a_max_len = text_len
                        
                        words = jieba.cut(text_content)
                        filtered = [w for w in words if len(w.strip()) > 1 and w.strip().lower() not in stop_words]
                        ai_counter.update(filtered)

    # é‡Šæ”¾ JSON å¯¹è±¡å ç”¨çš„å·¨å¤§å†…å­˜
    del data
    gc.collect()
    
    u_avg = int(u_total_len / u_count) if u_count > 0 else 0
    a_avg = int(a_total_len / a_count) if a_count > 0 else 0
    
    return user_counter, ai_counter, user_timeline, u_count, u_avg, u_max_len, a_count, a_avg, a_max_len

# ==========================================
# 4. é¢œè‰²æˆªæ–­å™¨
# ==========================================
def get_truncated_cmap(cmap_name, min_val=0.0, max_val=1.0, n=256):
    cmap = plt.get_cmap(cmap_name)
    new_cmap = mcolors.LinearSegmentedColormap.from_list(
        f'trunc({cmap_name},{min_val:.2f},{max_val:.2f})',
        cmap(np.linspace(min_val, max_val, n))
    )
    return new_cmap

# ==========================================
# 5. ç•Œé¢ä¾§è¾¹æ 
# ==========================================
wordcloud_colormaps = {"Blues (è“)": "Blues", "Oranges (æ©™)": "Oranges", "Reds (çº¢)": "Reds", "Greens (ç»¿)": "Greens", "Purples (ç´«)": "Purples", "viridis (æå…‰)": "viridis", "magma (å²©æµ†)": "magma", "cool (å†·è‰²)": "cool", "autumn (ç§‹è‰²)": "autumn"}
USER_ICON = "ğŸ‘¾" 
AI_ICON = "ğŸ¦¾"

with st.sidebar:
    st.markdown("<h1>âš™ï¸ è®¾ç½®é¢æ¿ v28.0</h1>", unsafe_allow_html=True)
    uploaded_file = st.file_uploader("1. ä¸Šä¼  conversations.json", type=['json'])
    
    st.markdown("---")
    st.header("ğŸ¨ è§†è§‰å®šåˆ¶")
    
    st.write("ğŸŒˆ **é¢œè‰²æµ“åº¦**")
    color_intensity = st.slider("å»é™¤æµ…è‰²æ¯”ä¾‹", 0.0, 0.6, 0.3)

    st.markdown("---")
    c1, c2 = st.columns(2)
    with c1:
        st.markdown(f"<h4>{USER_ICON} ä½ </h4>", unsafe_allow_html=True)
        user_wc_color = st.selectbox("ä½ çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=0)
        
    with c2:
        st.markdown(f"<h4>{AI_ICON} AI</h4>", unsafe_allow_html=True)
        ai_wc_color = st.selectbox("AI çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=1)

    st.markdown("---")
    max_words_limit = st.slider("ğŸ“Š æ•°æ®æ˜¾ç¤ºæ•°é‡", 20, 500, 100)
    
    st.markdown("---")
    st.header("ğŸ›¡ï¸ è¿‡æ»¤è®¾ç½®")
    use_default = st.toggle("ä½¿ç”¨å†…ç½®å‡€åŒ–è¯è¡¨", value=True)
    custom_input = st.text_area("è‡ªå®šä¹‰å±è”½è¯", height=80)
    
    final_stopwords = set(DEFAULT_STOPWORDS) if use_default else set()
    if custom_input: final_stopwords.update([w.strip().lower() for w in re.split(r'[ ,ï¼Œ\n]+', custom_input) if w.strip()])

# ==========================================
# 6. è¯äº‘é¢æ¿ (ç›´æ¥æ¥æ”¶ Counter)
# ==========================================
def show_wordcloud_panel(word_counts, cmap_name, title, icon, limit, min_val):
    if not word_counts: return
    
    base_cmap_name = wordcloud_colormaps[cmap_name]
    custom_cmap = get_truncated_cmap(base_cmap_name, min_val=min_val, max_val=1.0)
    fp = get_custom_font_path()

    try:
        wc = WordCloud(
            font_path=fp, 
            width=1000, height=1000,
            background_color='white', 
            colormap=custom_cmap, 
            max_words=limit, 
            contour_width=0
        ).generate_from_frequencies(word_counts) # ç›´æ¥ä½¿ç”¨ç»Ÿè®¡å¥½çš„é¢‘ç‡
        
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e: st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    with st.expander(f"ğŸ“‹ æŸ¥çœ‹ {icon} {title} é«˜é¢‘è¯è¡¨", expanded=False):
        st.dataframe(pd.DataFrame(word_counts.most_common(limit), columns=['è¯è¯­', 'æ¬¡æ•°']), use_container_width=True, height=300)

# ==========================================
# 7. æŸ±çŠ¶å›¾é¢æ¿ (ç›´æ¥æ¥æ”¶ Counter)
# ==========================================
def show_barchart_panel(word_counts, cmap_name, plain_text_title, limit):
    if not word_counts: return
    
    # ç›´æ¥ä» Counter å–å‰Nåï¼Œä¸éœ€è¦å†é‡æ–°ç»Ÿè®¡
    common_words = word_counts.most_common(limit)
    df = pd.DataFrame(common_words, columns=['Word', 'Count']).sort_values(by='Count', ascending=True)
    
    height_per_row = 0.45
    dynamic_height = max(6, len(df) * height_per_row)
    fig, ax = plt.subplots(figsize=(12, dynamic_height))
    
    font_normal = get_custom_font_prop(size=14)
    font_title = get_custom_font_prop(size=35, weight='bold')
    
    base_cmap_name = wordcloud_colormaps[cmap_name]
    cmap = get_truncated_cmap(base_cmap_name, 0.3, 0.9)
    
    for i, (word, count) in enumerate(zip(df['Word'], df['Count'])):
        gradient = np.linspace(0, 1, 256).reshape(1, -1)
        ax.imshow(gradient, aspect='auto', cmap=cmap, extent=(0, count, i - 0.3, i + 0.3))
        ax.text(count + (df['Count'].max() * 0.01), i, str(count), va='center', fontsize=12)

    ax.set_yticks(range(len(df)))
    ax.set_yticklabels(df['Word'], fontproperties=font_normal)
    ax.set_title(f"{plain_text_title} Top {limit} è¯é¢‘ç»Ÿè®¡", pad=40, fontproperties=font_title)
    
    ax.set_ylim(-0.5, len(df) - 0.5) 
    ax.set_xlim(0, df['Count'].max() * 1.15) 
    
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.grid(axis='x', linestyle='--', alpha=0.2)
    ax.tick_params(axis='x', labelsize=10)
    
    plt.tight_layout()
    st.pyplot(fig)

# ==========================================
# 8. æ—¶å…‰æœº (ä¼˜åŒ–ç‰ˆ)
# ==========================================
def show_timeline_panel(timeline_list):
    st.markdown("### ğŸ“… æœˆåº¦è¯é¢˜æ—¶å…‰æœº (æ·±åº¦å»å™ª)")
    st.caption("å·²è‡ªåŠ¨å‰”é™¤å…¨å±€æœ€å¸¸ç”¨çš„ 50 ä¸ªè¯ï¼Œåªæ˜¾ç¤ºæ¯æœˆçš„ç‹¬ç‰¹è¯é¢˜ã€‚")
    
    if not timeline_list: 
        st.warning("æ²¡æœ‰è§£æåˆ°æ—¶é—´æ•°æ®ã€‚")
        return
        
    # 1. æ„å»ºå…¨å±€è¯é¢‘ (ç”¨äºå»å™ª)
    all_words = []
    for item in timeline_list:
        all_words.extend(item['keywords'])
    
    global_counter = Counter(all_words)
    global_noise_words = set([w for w, c in global_counter.most_common(50)])
    
    # 2. æ„å»º DataFrame
    df = pd.DataFrame(timeline_list)
    df['month'] = df['time'].dt.to_period('M')
    
    # 3. æŒ‰æœˆèšåˆ
    timeline_data = []
    for month, group in df.groupby('month'):
        # æ±‡æ€»è¯¥æœˆæ‰€æœ‰å…³é”®è¯
        month_words = []
        for keywords in group['keywords']:
            month_words.extend(keywords)
            
        # è¿‡æ»¤å™ªéŸ³
        filtered = [w for w in month_words if w not in global_noise_words]
        
        top_n = Counter(filtered).most_common(10)
        top_str = " | ".join([f"{w}" for w, c in top_n])
        timeline_data.append({"æœˆä»½": str(month), "æœ¬æœˆç‰¹è‰²è¯é¢˜ (Top 10)": top_str, "å¯¹è¯æ¡æ•°": len(group)})
    
    df_timeline = pd.DataFrame(timeline_data).sort_values(by="æœˆä»½", ascending=False)
    st.dataframe(df_timeline, use_container_width=True, height=600)

# ==========================================
# ä¸»ç•Œé¢
# ==========================================
st.title("ğŸ›¸ ChatGPT æ·±åº¦åˆ†æ 28.0")

if uploaded_file:
    # è°ƒç”¨æ–°çš„è§£æå‡½æ•°ï¼Œè·å– 9 ä¸ªè¿”å›å€¼
    u_counter, a_counter, u_timeline, u_cnt, u_avg, u_max, a_cnt, a_avg, a_max = parse_and_count(uploaded_file, final_stopwords)
    
    if u_counter:
        st.markdown("### ğŸ§¬ èŠå¤©åŸºå› æŠ¥å‘Š")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric(f"{USER_ICON} ä½ çš„æ€»å‘è¨€", f"{u_cnt} æ¡")
        col2.metric(f"{USER_ICON} ä½ çš„å¹³å‡é•¿åº¦", f"{u_avg} å­—/æ¡")
        col3.metric(f"{AI_ICON} AI çš„æ€»å›å¤", f"{a_cnt} æ¡")
        col4.metric(f"{AI_ICON} AI çš„å¹³å‡é•¿åº¦", f"{a_avg} å­—/æ¡", delta=f"{a_avg - u_avg} (è¡¨è¾¾æ¬²)" if a_avg > u_avg else None)
        st.markdown("---")

        tab1, tab2, tab3 = st.tabs(["ğŸ¨ è¯äº‘ & è¯è¡¨", "ğŸ“Š å®åŠ›å¯¹æ¯”", "ğŸ“… æ—¶å…‰æœº"])
        
        with tab1:
            c1, c2 = st.columns(2)
            with c1: st.subheader(f"{USER_ICON} ä½ çš„è¯äº‘"); show_wordcloud_panel(u_counter, user_wc_color, "ç”¨æˆ·", USER_ICON, max_words_limit, color_intensity)
            with c2: st.subheader(f"{AI_ICON} AI çš„è¯äº‘"); show_wordcloud_panel(a_counter, ai_wc_color, "AI", AI_ICON, max_words_limit, color_intensity)
        with tab2:
            c1, c2 = st.columns(2)
            with c1: show_barchart_panel(u_counter, user_wc_color, "ç”¨æˆ·", max_words_limit)
            with c2: show_barchart_panel(a_counter, ai_wc_color, "AI", max_words_limit)
        with tab3: 
            show_timeline_panel(u_timeline)
else: st.write("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶å¼€å§‹")