import streamlit as st
# ç¡®ä¿è¿™äº› import éƒ½åœ¨æœ€ä¸Šé¢
import ijson
import pandas as pd  # è¿™é‡Œå°±æ˜¯æŠ¥é”™æ‰¾ä¸åˆ°çš„ pdï¼Œä¸€å®šè¦æœ‰ï¼
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.font_manager as fm
from collections import Counter, defaultdict
import re
import numpy as np
from datetime import datetime
import os
import gc

# ==========================================
# 0. åŸºç¡€é…ç½®
# ==========================================
st.set_page_config(page_title="ChatGPT æ·±åº¦åˆ†æ 31.0", layout="wide", page_icon="ğŸ“Š")

st.markdown("""
<style>
div[data-testid="stColorPicker"] {
    display: flex;
    justify-content: center;
    align-items: center;
    flex-direction: column;
    width: 100%;
}
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. å­—ä½“åŠ è½½å™¨
# ==========================================
@st.cache_resource
def get_custom_font_path():
    font_path = "msyh.ttc"
    if os.path.exists(font_path):
        return font_path
    else:
        return "arial.ttf" 

def get_custom_font_prop(size=14, weight='normal'):
    fp = get_custom_font_path()
    prop = fm.FontProperties(fname=fp)
    prop.set_size(size)
    prop.set_weight(weight)
    return prop

# ==========================================
# 2. å†…ç½®åœç”¨è¯è¡¨
# ==========================================
DEFAULT_STOPWORDS = {
    "---", "...", "###", "___", "***", "=>", "->", "<-", "<=", ">=", "**", "__", "~~", "==", "!=", "&&", "||", "```", "`", ">", "|", "[", "]", "{", "}", "(", ")",
    "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£", "å—", "å§", "å•Š", "å¯ä»¥", "è¿™ä¸ª", "é‚£ä¸ª", "å…¶å®", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "ä½†æ˜¯", 
    "å—¯", "å“¦", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸º", "ä¹‹", "ä¸", "åŠ", "å…¶", "ä»¥", "å¯¹", "è€Œ", "ç€", "ä¸‹", "åœ°", "å­", "ä¸­", "å¥¹", "ä»–", "ä»¬", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å®ƒä»¬", 
    "å“ˆå“ˆ", "å“ˆå“ˆå“ˆ", "å˜¿å˜¿", "å‘ƒ", "å˜›", "å‘€", "å‘¢", "å•¦", "å“‡", "å”‰", "å“", "å“¼", "å™¢", "å‘—",
    "å“ªæ€•", "å¯èƒ½", "è§‰å¾—", "åº”è¯¥", "è¿™ç§", "é‚£ç§", "æ¯”å¦‚", "æˆ–è€…", "ä»¥åŠ", "ç„¶å", "è™½ç„¶", "ä½†æ˜¯", "ä¸è¿‡", "åªæ˜¯", "è¿™æ ·", "é‚£æ ·", "è¿™é‡Œ", "é‚£é‡Œ",
    "ç¡®å®", "çœŸçš„", "éå¸¸", "ç‰¹åˆ«", "æ¯”è¾ƒ", "ç¨å¾®", "åªè¦", "åªæœ‰", "é™¤é", "å‡ ä¹", "ç®€ç›´", "ä¼¼ä¹", "å¥½åƒ", "å¤§æ¦‚", "ä¹Ÿè®¸",
    "1", "2", "3", "4", "5", "6", "7", "8", "9", "0", ".", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "â€œ", "â€", "ï¼š", "ï¼›", "ï¼ˆ", "ï¼‰", "ã€Š", "ã€‹", "â€¦", "â€”", "ï½",
    "the", "a", "an", "and", "or", "but", "if", "because", "as", "what", "when", "where", "how", "why", "which", "who", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do", "does", "did", "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "my", "your", "his", "its", "our", "their", "this", "that", "these", "those", "there", "here", "in", "on", "at", "to", "for", "of", "with", "by", "from", "up", "down", "out", "about", "into", "over", "after", "can", "could", "will", "would", "should", "may", "might", "must", "not", "no", "yes", "ok", "okay", "well", "just", "only", "very", "really", "too", "so", "much", "many", "some", "any", "all"
}

# ==========================================
# 3. æ ¸å¿ƒè§£æå‡½æ•° (æµå¼è¯»å–ä¿®å¤ç‰ˆ)
# ==========================================
@st.cache_data
def parse_and_count_stream(file, stop_words):
    try:
        # ã€æ–°å¢ã€‘é‡è¦ï¼šå°†æ–‡ä»¶æŒ‡é’ˆé‡ç½®åˆ°å¼€å¤´ï¼Œé˜²æ­¢å¤šæ¬¡è¯»å–æ—¶å‡ºé”™
        file.seek(0) 
        
        # ijson æµå¼è¯»å–
        # 'item' è¡¨ç¤ºæ ¹åˆ—è¡¨ä¸‹çš„æ¯ä¸€é¡¹
        conversations = ijson.items(file, 'item')
        
        user_counter = Counter()
        ai_counter = Counter()
        timeline_counters = defaultdict(Counter)
        timeline_counts = defaultdict(int)

        u_count = 0
        u_total_len = 0
        a_count = 0
        a_total_len = 0

        for conversation in conversations:
            mapping = conversation.get('mapping', {})
            create_time = conversation.get('create_time')
            base_dt = datetime.fromtimestamp(create_time) if create_time else None
            
            for node_id, node_data in mapping.items():
                message = node_data.get('message')
                # é˜²å¾¡æ€§æ£€æŸ¥
                if message is None: continue
                    
                if message and message.get('content') and message.get('author'):
                    role = message['author']['role']
                    content_parts = message['content'].get('parts', [])
                    text_content = "".join([part for part in content_parts if isinstance(part, str)])
                    
                    if text_content:
                        text_len = len(text_content)
                        msg_time = message.get('create_time')
                        dt = datetime.fromtimestamp(msg_time) if msg_time else base_dt
                        month_key = dt.strftime('%Y-%m') if dt else "Unknown"

                        if role == 'user':
                            u_count += 1
                            u_total_len += text_len
                            words = jieba.cut(text_content)
                            filtered = [w for w in words if len(w.strip()) > 1 and w.strip().lower() not in stop_words]
                            user_counter.update(filtered)
                            if month_key != "Unknown":
                                timeline_counters[month_key].update(filtered)
                                timeline_counts[month_key] += 1

                        elif role == 'assistant':
                            a_count += 1
                            a_total_len += text_len
                            words = jieba.cut(text_content)
                            filtered = [w for w in words if len(w.strip()) > 1 and w.strip().lower() not in stop_words]
                            ai_counter.update(filtered)
        
        # è®¡ç®—å¹³å‡å€¼
        u_avg = int(u_total_len / u_count) if u_count > 0 else 0
        a_avg = int(a_total_len / a_count) if a_count > 0 else 0
        
        # æ˜¾å¼åƒåœ¾å›æ”¶
        del conversations
        gc.collect()

        return {
            "u_counter": user_counter,
            "a_counter": ai_counter,
            "timeline_counters": timeline_counters,
            "timeline_counts": timeline_counts,
            "u_count": u_count, "u_avg": u_avg,
            "a_count": a_count, "a_avg": a_avg
        }
                            
    except Exception as e:
        st.error(f"è§£æå‡ºé”™: {e}")
        return None

# ==========================================
# 4. é¢œè‰²æˆªæ–­å™¨
# ==========================================
def get_truncated_cmap(cmap_name, min_val=0.0, max_val=1.0, n=256):
    cmap = plt.get_cmap(cmap_name)
    new_cmap = mcolors.LinearSegmentedColormap.from_list(
        f'trunc({cmap_name},{min_val:.2f},{max_val:.2f})',
        cmap(np.linspace(min_val, max_val, n))
    )
    return new_cmap

# ==========================================
# 5. ç•Œé¢ä¾§è¾¹æ 
# ==========================================
wordcloud_colormaps = {"Blues (è“)": "Blues", "Oranges (æ©™)": "Oranges", "Reds (çº¢)": "Reds", "Greens (ç»¿)": "Greens", "Purples (ç´«)": "Purples", "viridis (æå…‰)": "viridis", "magma (å²©æµ†)": "magma", "cool (å†·è‰²)": "cool", "autumn (ç§‹è‰²)": "autumn"}
USER_ICON = "ğŸ‘¾" 
AI_ICON = "ğŸ¦¾"

with st.sidebar:
    st.header("âš™ï¸ è®¾ç½®é¢æ¿ v31.0")
    uploaded_file = st.file_uploader("1. ä¸Šä¼  conversations.json", type=['json'])
    
    st.markdown("---")
    st.header("ğŸ¨ è§†è§‰å®šåˆ¶")
    
    st.write("ğŸŒˆ **é¢œè‰²æµ“åº¦**")
    color_intensity = st.slider("å»é™¤æµ…è‰²æ¯”ä¾‹", 0.0, 0.6, 0.3)

    st.markdown("---")
    c1, c2 = st.columns(2)
    with c1:
        st.subheader(f"{USER_ICON} ä½ ")
        user_wc_color = st.selectbox("ä½ çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=0)
        
    with c2:
        st.subheader(f"{AI_ICON} AI")
        ai_wc_color = st.selectbox("AI çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=1)

    st.markdown("---")
    max_words_limit = st.slider("ğŸ“Š æ•°æ®æ˜¾ç¤ºæ•°é‡", 20, 500, 100)
    
    st.markdown("---")
    st.header("ğŸ›¡ï¸ è¿‡æ»¤è®¾ç½®")
    use_default = st.toggle("ä½¿ç”¨å†…ç½®å‡€åŒ–è¯è¡¨", value=True)
    custom_input = st.text_area("è‡ªå®šä¹‰å±è”½è¯", height=80)
    
    final_stopwords = set(DEFAULT_STOPWORDS) if use_default else set()
    if custom_input: final_stopwords.update([w.strip().lower() for w in re.split(r'[ ,ï¼Œ\n]+', custom_input) if w.strip()])

# ==========================================
# 6. è¯äº‘é¢æ¿
# ==========================================
def show_wordcloud_panel(word_counts, cmap_name, title, icon, limit, min_val):
    if not word_counts: return
    
    base_cmap_name = wordcloud_colormaps[cmap_name]
    custom_cmap = get_truncated_cmap(base_cmap_name, min_val=min_val, max_val=1.0)
    fp = get_custom_font_path()

    try:
        wc = WordCloud(
            font_path=fp, 
            width=1000, height=1000,
            background_color='white', 
            colormap=custom_cmap, 
            max_words=limit, 
            contour_width=0
        ).generate_from_frequencies(word_counts)
        
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e: st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    # è¿™é‡Œçš„ pd å¿…é¡»åœ¨é¡¶éƒ¨ import pandas as pd
    with st.expander(f"ğŸ“‹ æŸ¥çœ‹ {icon} {title} é«˜é¢‘è¯è¡¨", expanded=False):
        st.dataframe(pd.DataFrame(word_counts.most_common(limit), columns=['è¯è¯­', 'æ¬¡æ•°']), use_container_width=True, height=300)

# ==========================================
# 7. æŸ±çŠ¶å›¾é¢æ¿
# ==========================================
def show_barchart_panel(word_counts, cmap_name, plain_text_title, limit):
    if not word_counts: return
    
    common_words = word_counts.most_common(limit)
    df = pd.DataFrame(common_words, columns=['Word', 'Count']).sort_values(by='Count', ascending=True)
    
    height_per_row = 0.45
    dynamic_height = max(6, len(df) * height_per_row)
    fig, ax = plt.subplots(figsize=(12, dynamic_height))
    
    font_normal = get_custom_font_prop(size=14)
    font_title = get_custom_font_prop(size=35, weight='bold')
    
    base_cmap_name = wordcloud_colormaps[cmap_name]
    cmap = get_truncated_cmap(base_cmap_name, 0.3, 0.9)
    
    for i, (word, count) in enumerate(zip(df['Word'], df['Count'])):
        gradient = np.linspace(0, 1, 256).reshape(1, -1)
        ax.imshow(gradient, aspect='auto', cmap=cmap, extent=(0, count, i - 0.3, i + 0.3))
        ax.text(count + (df['Count'].max() * 0.01), i, str(count), va='center', fontsize=12)

    ax.set_yticks(range(len(df)))
    ax.set_yticklabels(df['Word'], fontproperties=font_normal)
    ax.set_title(f"{plain_text_title} Top {limit} è¯é¢‘ç»Ÿè®¡", pad=40, fontproperties=font_title)
    
    ax.set_ylim(-0.5, len(df) - 0.5) 
    ax.set_xlim(0, df['Count'].max() * 1.15) 
    
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.grid(axis='x', linestyle='--', alpha=0.2)
    ax.tick_params(axis='x', labelsize=10)
    
    plt.tight_layout()
    st.pyplot(fig)

# ==========================================
# 8. æ—¶å…‰æœº (æ·±åº¦å»å™ª)
# ==========================================
def show_timeline_panel(res):
    st.markdown("### ğŸ“… æœˆåº¦è¯é¢˜æ—¶å…‰æœº (æ·±åº¦å»å™ª)")
    st.caption("å·²è‡ªåŠ¨å‰”é™¤å…¨å±€æœ€å¸¸ç”¨çš„ 50 ä¸ªè¯ï¼Œåªæ˜¾ç¤ºæ¯æœˆçš„ç‹¬ç‰¹è¯é¢˜ã€‚")
    
    timeline_counters = res["timeline_counters"]
    timeline_counts = res["timeline_counts"]
    
    if not timeline_counters: 
        st.warning("æ²¡æœ‰è§£æåˆ°æ—¶é—´æ•°æ®ã€‚")
        return
        
    # 1. è®¡ç®—å…¨å±€å™ªéŸ³ (Top 50)
    global_counter = Counter()
    for c in timeline_counters.values():
        global_counter.update(c)
    global_noise_words = set([w for w, c in global_counter.most_common(50)])
    
    timeline_data = []
    sorted_months = sorted(timeline_counters.keys(), reverse=True)
    
    for month in sorted_months:
        month_counter = timeline_counters[month]
        count = timeline_counts[month]
        
        filtered_counter = Counter()
        for w, c in month_counter.items():
            if w not in global_noise_words:
                filtered_counter[w] = c
        
        top_n = filtered_counter.most_common(10)
        top_str = " | ".join([f"{w}" for w, c in top_n])
        
        timeline_data.append({
            "æœˆä»½": month, 
            "æœ¬æœˆç‰¹è‰²è¯é¢˜ (Top 10)": top_str, 
            "å¯¹è¯æ¡æ•°": count
        })
    
    df_timeline = pd.DataFrame(timeline_data)
    st.dataframe(df_timeline, use_container_width=True, height=600)

# ==========================================
# ä¸»ç•Œé¢
# ==========================================
st.title("ğŸ›¸ ChatGPT æ·±åº¦åˆ†æ 31.0")

if uploaded_file:
    # è°ƒç”¨è§£æ
    res = parse_and_count_stream(uploaded_file, final_stopwords)
    
    if res:
        st.markdown("### ğŸ§¬ èŠå¤©åŸºå› æŠ¥å‘Š")
        col1, col2, col3, col4 = st.columns(4)
        col1.metric(f"{USER_ICON} ä½ çš„æ€»å‘è¨€", f"{res['u_count']} æ¡")
        col2.metric(f"{USER_ICON} ä½ çš„å¹³å‡é•¿åº¦", f"{res['u_avg']} å­—/æ¡")
        col3.metric(f"{AI_ICON} AI çš„æ€»å›å¤", f"{res['a_count']} æ¡")
        delta_val = res['a_avg'] - res['u_avg']
        col4.metric(f"{AI_ICON} AI çš„å¹³å‡é•¿åº¦", f"{res['a_avg']} å­—/æ¡", delta=f"{delta_val} (è¡¨è¾¾æ¬²)" if delta_val > 0 else None)
        st.markdown("---")

        tab1, tab2, tab3 = st.tabs(["ğŸ¨ è¯äº‘ & è¯è¡¨", "ğŸ“Š å®åŠ›å¯¹æ¯”", "ğŸ“… æ—¶å…‰æœº"])
        
        with tab1:
            c1, c2 = st.columns(2)
            with c1: st.subheader(f"{USER_ICON} ä½ çš„è¯äº‘"); show_wordcloud_panel(res['u_counter'], user_wc_color, "ç”¨æˆ·", USER_ICON, max_words_limit, color_intensity)
            with c2: st.subheader(f"{AI_ICON} AI çš„è¯äº‘"); show_wordcloud_panel(res['a_counter'], ai_wc_color, "AI", AI_ICON, max_words_limit, color_intensity)
        with tab2:
            c1, c2 = st.columns(2)
            with c1: show_barchart_panel(res['u_counter'], user_wc_color, "ç”¨æˆ·", max_words_limit)
            with c2: show_barchart_panel(res['a_counter'], ai_wc_color, "AI", max_words_limit)
        with tab3: 
            show_timeline_panel(res)
else: st.write("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶å¼€å§‹")