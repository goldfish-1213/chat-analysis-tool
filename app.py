import streamlit as st
import json
import pandas as pd
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.font_manager as fm
from collections import Counter
import re
import numpy as np
from PIL import Image, ImageFont, ImageDraw
from datetime import datetime
import os
import urllib.request # ä½¿ç”¨è‡ªå¸¦åº“ä¸‹è½½å›¾ç‰‡ï¼Œæ— éœ€æ›´æ–° requirements.txt

# ==========================================
# 0. åŸºç¡€é…ç½® & CSS
# ==========================================
st.set_page_config(page_title="ChatGPT æ·±åº¦åˆ†æ 22.0", layout="wide", page_icon="ğŸ¦Š")

st.markdown("""
<style>
div[data-testid="stColorPicker"] {
    display: flex;
    justify-content: center;
    align-items: center;
    flex-direction: column;
    width: 100%;
}
div[data-testid="stColorPicker"] > label {
    width: 100%;
    text-align: center;
    font-weight: bold;
}
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. å­—ä½“åŠ è½½å™¨
# ==========================================
def get_custom_font_path():
    font_path = "msyh.ttc"
    if os.path.exists(font_path):
        return font_path
    else:
        return "arial.ttf" 

def get_custom_font_prop():
    fp = get_custom_font_path()
    return fm.FontProperties(fname=fp)

# ==========================================
# 2. å†…ç½®åœç”¨è¯è¡¨
# ==========================================
DEFAULT_STOPWORDS = {
    "---", "...", "###", "___", "***", "=>", "->", "<-", "<=", ">=", "**", "__", "~~", "==", "!=", "&&", "||", "```", "`", ">", "|", "[", "]", "{", "}", "(", ")",
    "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£", "å—", "å§", "å•Š", "å¯ä»¥", "è¿™ä¸ª", "é‚£ä¸ª", "å…¶å®", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "ä½†æ˜¯", 
    "å—¯", "å“¦", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸º", "ä¹‹", "ä¸", "åŠ", "å…¶", "ä»¥", "å¯¹", "è€Œ", "ç€", "ä¸‹", "åœ°", "å­", "ä¸­", "å¥¹", "ä»–", "ä»¬", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å®ƒä»¬", 
    "å“ˆå“ˆ", "å“ˆå“ˆå“ˆ", "å˜¿å˜¿", "å‘ƒ", "å˜›", "å‘€", "å‘¢", "å•¦", "å“‡", "å”‰", "å“", "å“¼", "å™¢", "å‘—",
    "å“ªæ€•", "å¯èƒ½", "è§‰å¾—", "åº”è¯¥", "è¿™ç§", "é‚£ç§", "æ¯”å¦‚", "æˆ–è€…", "ä»¥åŠ", "ç„¶å", "è™½ç„¶", "ä½†æ˜¯", "ä¸è¿‡", "åªæ˜¯", "è¿™æ ·", "é‚£æ ·", "è¿™é‡Œ", "é‚£é‡Œ",
    "ç¡®å®", "çœŸçš„", "éå¸¸", "ç‰¹åˆ«", "æ¯”è¾ƒ", "ç¨å¾®", "åªè¦", "åªæœ‰", "é™¤é", "å‡ ä¹", "ç®€ç›´", "ä¼¼ä¹", "å¥½åƒ", "å¤§æ¦‚", "ä¹Ÿè®¸",
    "1", "2", "3", "4", "5", "6", "7", "8", "9", "0", ".", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "â€œ", "â€", "ï¼š", "ï¼›", "ï¼ˆ", "ï¼‰", "ã€Š", "ã€‹", "â€¦", "â€”", "ï½",
    "the", "a", "an", "and", "or", "but", "if", "because", "as", "what", "when", "where", "how", "why", "which", "who", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do", "does", "did", "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "my", "your", "his", "its", "our", "their", "this", "that", "these", "those", "there", "here", "in", "on", "at", "to", "for", "of", "with", "by", "from", "up", "down", "out", "about", "into", "over", "after", "can", "could", "will", "would", "should", "may", "might", "must", "not", "no", "yes", "ok", "okay", "well", "just", "only", "very", "really", "too", "so", "much", "many", "some", "any", "all"
}

# ==========================================
# 3. æ ¸å¿ƒè§£æå‡½æ•°
# ==========================================
@st.cache_data
def parse_data(file):
    try:
        data = json.load(file)
    except:
        st.error("æ–‡ä»¶æ ¼å¼ä¸å¯¹ï¼Œè¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯ JSON æ–‡ä»¶")
        return [], [], []
    
    user_data_list = []
    ai_data_list = [] 
    
    for conversation in data:
        mapping = conversation.get('mapping', {})
        create_time = conversation.get('create_time')
        base_dt = datetime.fromtimestamp(create_time) if create_time else None
        
        for node_id, node_data in mapping.items():
            message = node_data.get('message')
            if message and message.get('content') and message.get('author'):
                role = message['author']['role']
                msg_time = message.get('create_time')
                dt = datetime.fromtimestamp(msg_time) if msg_time else base_dt
                content_parts = message['content'].get('parts', [])
                text_content = "".join([part for part in content_parts if isinstance(part, str)])
                
                if text_content and dt:
                    item = {"text": text_content, "time": dt}
                    if role == 'user': user_data_list.append(item)
                    elif role == 'assistant': ai_data_list.append(item)
    return user_data_list, ai_data_list

# ==========================================
# 4. ç»Ÿè®¡å‡½æ•°
# ==========================================
def calculate_stats(data_list):
    if not data_list: return 0, 0, 0
    lengths = [len(d['text']) for d in data_list]
    total_len = sum(lengths)
    avg_len = total_len / len(lengths)
    max_len = max(lengths)
    return len(data_list), int(avg_len), max_len

# ==========================================
# 5. å½¢çŠ¶ç”Ÿæˆå™¨ (æ··åˆæ¨¡å¼ï¼šå­—ä½“ + ç½‘ç»œå›¾ç‰‡é»‘ç§‘æŠ€)
# ==========================================
@st.cache_data
def get_mask(emoji_str):
    char = emoji_str.split(" ")[0]
    
    # 1. ç®€å•å‡ ä½•å›¾å½¢ï¼šç”¨ä»£ç ç”» (æœ€å¿«)
    if char == "â—¼ï¸": return None
    if char == "â¤ï¸":
        x, y = np.ogrid[:300, :300]
        mask = np.zeros((300, 300), dtype=np.uint8) + 255
        xc, yc = (x - 150)/65.0, (y - 150)/65.0
        mask[(xc**2 + yc**2 - 1)**3 - (xc**2) * (yc**3) < 0] = 0
        return mask
    if char == "ğŸŸ¢":
        x, y = np.ogrid[:300, :300]
        mask = np.zeros((300, 300), dtype=np.uint8) + 255
        mask[(x - 150)**2 + (y - 150)**2 < 140**2] = 0
        return mask

    # 2. å¤æ‚åŠ¨ç‰©/ç‰©ä½“ï¼šä» Google Noto Emoji ä»“åº“ä¸‹è½½é«˜æ¸…å›¾
    # è¿™äº›é“¾æ¥æ˜¯ Google å®˜æ–¹å¼€æºçš„ Emoji å›¾ç‰‡
    url_map = {
        "â˜ï¸": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u2601.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u2601.png)", # äº‘
        "ğŸ’¬": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f4ac.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f4ac.png)", # æ°”æ³¡
        "ğŸ¦Š": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f98a.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f98a.png)", # ç‹ç‹¸
        "ğŸŸ": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f41f.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f41f.png)", # é±¼
        "ğŸ±": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f431.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f431.png)", # çŒ«
        "ğŸ¦‰": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f989.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f989.png)", # çŒ«å¤´é¹°
        "ğŸ¶": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f436.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f436.png)", # ç‹—
        "ğŸ§ ": "[https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f9e0.png](https://raw.githubusercontent.com/googlefonts/noto-emoji/main/png/512/emoji_u1f9e0.png)", # å¤§è„‘
    }
    
    url = url_map.get(char)
    if not url: return None # æ‰¾ä¸åˆ°å°±è¿”å›çŸ©å½¢

    try:
        # ä¸‹è½½å›¾ç‰‡
        with urllib.request.urlopen(url) as response:
            img_data = response.read()
        
        img = Image.open(BytesIO(img_data))
        
        # ã€æ ¸å¿ƒé»‘ç§‘æŠ€ã€‘å°†å½©è‰² Emoji è½¬æ¢ä¸ºé»‘ç™½å‰ªå½± Mask
        # 1. åˆ›å»ºä¸€ä¸ªçº¯ç™½åº•å›¾
        bg = Image.new("RGB", img.size, (255, 255, 255))
        # 2. æå– Alpha é€šé“ (é€æ˜åº¦)
        if img.mode != 'RGBA':
            img = img.convert('RGBA')
        alpha = img.split()[3]
        
        # 3. å°† Alpha é€šé“ä½œä¸ºè’™ç‰ˆï¼ŒæŠŠé»‘è‰²è´´ä¸Šå»
        # æ„æ€ï¼šå‡¡æ˜¯åŸå›¾ä¸é€æ˜çš„åœ°æ–¹ï¼Œéƒ½å¡«æˆé»‘è‰²
        mask_layer = Image.new("RGB", img.size, (0, 0, 0))
        bg.paste(mask_layer, mask=alpha)
        
        # 4. è½¬ç°åº¦å¹¶äºŒå€¼åŒ–ï¼Œç¡®ä¿åªæœ‰çº¯é»‘å’Œçº¯ç™½
        mask_array = np.array(bg.convert("L"))
        mask_array[mask_array < 200] = 0   # æœ‰é¢œè‰²çš„åœ°æ–¹å˜é»‘
        mask_array[mask_array >= 200] = 255 # èƒŒæ™¯å˜ç™½
        
        return mask_array
        
    except Exception as e:
        print(f"Error downloading mask: {e}")
        return None

# ==========================================
# 6. é¢œè‰²æˆªæ–­å™¨
# ==========================================
def get_truncated_cmap(cmap_name, min_val=0.0, max_val=1.0, n=256):
    cmap = plt.get_cmap(cmap_name)
    new_cmap = mcolors.LinearSegmentedColormap.from_list(
        f'trunc({cmap_name},{min_val:.2f},{max_val:.2f})',
        cmap(np.linspace(min_val, max_val, n))
    )
    return new_cmap

# ==========================================
# 7. ç•Œé¢ä¾§è¾¹æ 
# ==========================================
# åŠ¨ç‰©ä»¬å›æ¥å•¦ï¼
emoji_options = ["â—¼ï¸ çŸ©å½¢", "â˜ï¸ äº‘æœµ", "â¤ï¸ çˆ±å¿ƒ", "ğŸ’¬ æ°”æ³¡", "ğŸ§  å¤§è„‘", "ğŸ¦Š ç‹ç‹¸", "ğŸŸ å°é±¼", "ğŸ± çŒ«å’ª", "ğŸ¦‰ çŒ«å¤´é¹°", "ğŸ¶ å°ç‹—"]
wordcloud_colormaps = {"Blues (è“)": "Blues", "Oranges (æ©™)": "Oranges", "Reds (çº¢)": "Reds", "Greens (ç»¿)": "Greens", "Purples (ç´«)": "Purples", "viridis (æå…‰)": "viridis", "magma (å²©æµ†)": "magma", "cool (å†·è‰²)": "cool", "autumn (ç§‹è‰²)": "autumn"}
USER_ICON = "ğŸ‘¾" 
AI_ICON = "ğŸ¦¾"

with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®é¢æ¿ v22.0")
    uploaded_file = st.file_uploader("1. ä¸Šä¼  conversations.json", type=['json'])
    
    st.markdown("---")
    st.header("ğŸ¨ è§†è§‰å®šåˆ¶")
    
    st.write("ğŸŒˆ **é¢œè‰²æµ“åº¦**")
    color_intensity = st.slider("å»é™¤æµ…è‰²æ¯”ä¾‹", 0.0, 0.6, 0.3)

    st.markdown("---")
    c1, c2 = st.columns(2)
    with c1:
        st.markdown(f"<h4 style='text-align: center;'>{USER_ICON} ä½ </h4>", unsafe_allow_html=True)
        user_shape = st.selectbox("ä½ çš„å½¢çŠ¶", emoji_options, index=5) # é»˜è®¤é€‰ä¸ªç‹ç‹¸ç©ç©
        user_wc_color = st.selectbox("ä½ çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=0)
        
    with c2:
        st.markdown(f"<h4 style='text-align: center;'>{AI_ICON} AI</h4>", unsafe_allow_html=True)
        ai_shape = st.selectbox("AI çš„å½¢çŠ¶", emoji_options, index=3)
        ai_wc_color = st.selectbox("AI çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=1)

    st.markdown("---")
    max_words_limit = st.slider("ğŸ“Š æ•°æ®æ˜¾ç¤ºæ•°é‡", 20, 500, 100)
    
    st.markdown("---")
    st.header("ğŸ›¡ï¸ è¿‡æ»¤è®¾ç½®")
    use_default = st.toggle("ä½¿ç”¨å†…ç½®å‡€åŒ–è¯è¡¨", value=True)
    custom_input = st.text_area("è‡ªå®šä¹‰å±è”½è¯", height=80)
    final_stopwords = set(DEFAULT_STOPWORDS) if use_default else set()
    if custom_input: final_stopwords.update([w.strip().lower() for w in re.split(r'[ ,ï¼Œ\n]+', custom_input) if w.strip()])

# ==========================================
# 8. è¯äº‘é¢æ¿
# ==========================================
def show_wordcloud_panel(data_list, cmap_name, shape_str, title, icon, limit, min_val):
    if not data_list: return
    text_list = [d['text'] for d in data_list]
    full_text = " ".join(text_list)
    words = jieba.lcut(full_text)
    filtered_words = [w.strip() for w in words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    word_counts = Counter(filtered_words)
    
    mask = get_mask(shape_str)
    base_cmap_name = wordcloud_colormaps[cmap_name]
    custom_cmap = get_truncated_cmap(base_cmap_name, min_val=min_val, max_val=1.0)
    fp = get_custom_font_path()

    try:
        wc = WordCloud(font_path=fp, width=1000, height=1000, background_color='white', colormap=custom_cmap, max_words=limit, stopwords=final_stopwords, mask=mask, contour_width=0).generate_from_frequencies(word_counts)
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e: st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    with st.expander(f"ğŸ“‹ æŸ¥çœ‹ {icon} {title} é«˜é¢‘è¯è¡¨", expanded=False):
        st.dataframe(pd.DataFrame(word_counts.most_common(limit), columns=['è¯è¯­', 'æ¬¡æ•°']), use_container_width=True, height=300)

# ==========================================
# 9. æŸ±çŠ¶å›¾é¢æ¿
# ==========================================
def show_barchart_panel(data_list, cmap_name, role_icon, limit):
    if not data_list: return
    text_list = [d['text'] for d in data_list]
    words = [w.strip() for w in jieba.lcut(" ".join(text_list)) if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    common_words = Counter(words).most_common(limit)
    if not common_words: return
    
    df = pd.DataFrame(common_words, columns=['Word', 'Count']).sort_values(by='Count', ascending=True)
    height_per_row = 0.45
    dynamic_height = max(6, len(df) * height_per_row)
    fig, ax = plt.subplots(figsize=(12, dynamic_height))
    
    font_prop = get_custom_font_prop()
    base_cmap_name = wordcloud_colormaps[cmap_name]
    cmap = get_truncated_cmap(base_cmap_name, 0.3, 0.9)
    
    for i, (word, count) in enumerate(zip(df['Word'], df['Count'])):
        gradient = np.linspace(0, 1, 256).reshape(1, -1)
        ax.imshow(gradient, aspect='auto', cmap=cmap, extent=(0, count, i - 0.3, i + 0.3))
        ax.text(count + (df['Count'].max() * 0.01), i, str(count), va='center', fontsize=12)

    ax.set_yticks(range(len(df)))
    ax.set_yticklabels(df['Word'], fontsize=14, fontproperties=font_prop)
    ax.set_title(f"{role_icon} Top {limit} è¯é¢‘ç»Ÿè®¡", fontsize=20, pad=20, fontproperties=font_prop)
    
    ax.set_ylim(-0.5, len(df) - 0.5) 
    ax.set_xlim(0, df['Count'].max() * 1.15) 
    
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.grid(axis='x', linestyle='--', alpha=0.2)
    ax.tick_params(axis='x', labelsize=10)
    
    plt.tight_layout()
    st.pyplot(fig)

# ==========================================
# 10. æ—¶å…‰æœº
# ==========================================
def show_timeline_panel(user_list):
    st.markdown("### ğŸ“… æœˆåº¦è¯é¢˜æ—¶å…‰æœº (æ·±åº¦å»å™ª)")
    st.caption("å·²è‡ªåŠ¨å‰”é™¤å…¨å±€æœ€å¸¸ç”¨çš„ 50 ä¸ªè¯ï¼Œåªæ˜¾ç¤ºæ¯æœˆçš„ç‹¬ç‰¹è¯é¢˜ã€‚")
    
    df_u = pd.DataFrame(user_list)
    if df_u.empty: 
        st.warning("æ²¡æœ‰è§£æåˆ°æ—¶é—´æ•°æ®ã€‚")
        return
    
    all_text = " ".join(df_u['text'].tolist())
    all_words = jieba.lcut(all_text)
    all_filtered = [w.strip() for w in all_words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    global_counter = Counter(all_filtered)
    global_noise_words = set([w for w, c in global_counter.most_common(50)])
    
    df_u['month'] = df_u['time'].dt.to_period('M')
    monthly_groups = df_u.groupby('month')
    
    timeline_data = []
    for month, group in monthly_groups:
        month_text = " ".join(group['text'].tolist())
        words = jieba.lcut(month_text)
        filtered = [w.strip() for w in words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords and w.strip() not in global_noise_words]
        top_n = Counter(filtered).most_common(10) 
        top_str = " | ".join([f"{w}" for w, c in top_n])
        timeline_data.append({"æœˆä»½": str(month), "æœ¬æœˆç‰¹è‰²è¯é¢˜ (Top 10)": top_str, "å¯¹è¯æ¡æ•°": len(group)})
    
    df_timeline = pd.DataFrame(timeline_data).sort_values(by="æœˆä»½", ascending=False)
    st.dataframe(df_timeline, use_container_width=True, height=600)

# ==========================================
# ä¸»ç•Œé¢
# ==========================================
st.title("ğŸ›¸ ChatGPT æ·±åº¦åˆ†æ 22.0")

if uploaded_file:
    user_data, ai_data = parse_data(uploaded_file)
    
    u_count, u_avg, u_max = calculate_stats(user_data)
    a_count, a_avg, a_max = calculate_stats(ai_data)
    st.markdown("### ğŸ§¬ èŠå¤©åŸºå› æŠ¥å‘Š")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric(f"{USER_ICON} ä½ çš„æ€»å‘è¨€", f"{u_count} æ¡")
    col2.metric(f"{USER_ICON} ä½ çš„å¹³å‡é•¿åº¦", f"{u_avg} å­—/æ¡")
    col3.metric(f"{AI_ICON} AI çš„æ€»å›å¤", f"{a_count} æ¡")
    col4.metric(f"{AI_ICON} AI çš„å¹³å‡é•¿åº¦", f"{a_avg} å­—/æ¡", delta=f"{a_avg - u_avg} (è¡¨è¾¾æ¬²)" if a_avg > u_avg else None)
    st.markdown("---")

    tab1, tab2, tab3 = st.tabs(["ğŸ¨ è¯äº‘ & è¯è¡¨", "ğŸ“Š å®åŠ›å¯¹æ¯”", "ğŸ“… æ—¶å…‰æœº"])
    
    with tab1:
        c1, c2 = st.columns(2)
        with c1: st.subheader(f"{USER_ICON} ä½ çš„è¯äº‘"); show_wordcloud_panel(user_data, user_wc_color, user_shape, "ç”¨æˆ·", USER_ICON, max_words_limit, color_intensity)
        with c2: st.subheader(f"{AI_ICON} AI çš„è¯äº‘"); show_wordcloud_panel(ai_data, ai_wc_color, ai_shape, "AI", AI_ICON, max_words_limit, color_intensity)
    with tab2:
        c1, c2 = st.columns(2)
        with c1: show_barchart_panel(user_data, user_wc_color, USER_ICON, max_words_limit)
        with c2: show_barchart_panel(ai_data, ai_wc_color, AI_ICON, max_words_limit)
    with tab3: 
        show_timeline_panel(user_data)
else: st.write("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶å¼€å§‹")