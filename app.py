import streamlit as st
import json
import pandas as pd
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from collections import Counter
import re
import numpy as np
from PIL import Image, ImageFont, ImageDraw
from datetime import datetime

# ==========================================
# 0. åŸºç¡€é…ç½® & CSS
# ==========================================
st.set_page_config(page_title="ChatGPT æ·±åº¦åˆ†æ 19.0", layout="wide", page_icon="ğŸ›¸")
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS'] 
plt.rcParams['axes.unicode_minus'] = False

st.markdown("""
<style>
div[data-testid="stColorPicker"] {
    display: flex;
    justify-content: center;
    align-items: center;
    flex-direction: column;
    width: 100%;
}
div[data-testid="stColorPicker"] > label {
    width: 100%;
    text-align: center;
    font-weight: bold;
}
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. å†…ç½®åœç”¨è¯è¡¨
# ==========================================
DEFAULT_STOPWORDS = {
    "---", "...", "###", "___", "***", "=>", "->", "<-", "<=", ">=", "**", "__", "~~", "==", "!=", "&&", "||", "```", "`", ">", "|", "[", "]", "{", "}", "(", ")",
    "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£", "å—", "å§", "å•Š", "å¯ä»¥", "è¿™ä¸ª", "é‚£ä¸ª", "å…¶å®", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "ä½†æ˜¯", 
    "å—¯", "å“¦", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸º", "ä¹‹", "ä¸", "åŠ", "å…¶", "ä»¥", "å¯¹", "è€Œ", "ç€", "ä¸‹", "åœ°", "å­", "ä¸­", "å®ƒ", "å¥¹", "ä»–", "ä»¬", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å®ƒä»¬", 
    "å“ˆå“ˆ", "å“ˆå“ˆå“ˆ", "å˜¿å˜¿", "å‘ƒ", "å˜›", "å‘€", "å‘¢", "å•¦", "å“‡", "å”‰", "å“", "å“¼", "å™¢", "å‘—",
    "å“ªæ€•", "å¯èƒ½", "è§‰å¾—", "åº”è¯¥", "è¿™ç§", "é‚£ç§", "æ¯”å¦‚", "æˆ–è€…", "ä»¥åŠ", "ç„¶å", "è™½ç„¶", "ä½†æ˜¯", "ä¸è¿‡", "åªæ˜¯", "è¿™æ ·", "é‚£æ ·", "è¿™é‡Œ", "é‚£é‡Œ",
    "ç¡®å®", "çœŸçš„", "éå¸¸", "ç‰¹åˆ«", "æ¯”è¾ƒ", "ç¨å¾®", "åªè¦", "åªæœ‰", "é™¤é", "å‡ ä¹", "ç®€ç›´", "ä¼¼ä¹", "å¥½åƒ", "å¤§æ¦‚", "ä¹Ÿè®¸",
    "1", "2", "3", "4", "5", "6", "7", "8", "9", "0", ".", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "â€œ", "â€", "ï¼š", "ï¼›", "ï¼ˆ", "ï¼‰", "ã€Š", "ã€‹", "â€¦", "â€”", "ï½",
    "the", "a", "an", "and", "or", "but", "if", "because", "as", "what", "when", "where", "how", "why", "which", "who", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do", "does", "did", "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "my", "your", "his", "its", "our", "their", "this", "that", "these", "those", "there", "here", "in", "on", "at", "to", "for", "of", "with", "by", "from", "up", "down", "out", "about", "into", "over", "after", "can", "could", "will", "would", "should", "may", "might", "must", "not", "no", "yes", "ok", "okay", "well", "just", "only", "very", "really", "too", "so", "much", "many", "some", "any", "all"
}

# ==========================================
# 2. æ ¸å¿ƒè§£æå‡½æ•°
# ==========================================
@st.cache_data
def parse_data(file):
    try:
        data = json.load(file)
    except:
        st.error("æ–‡ä»¶æ ¼å¼ä¸å¯¹ï¼Œè¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯ JSON æ–‡ä»¶")
        return [], [], []
    
    user_data_list = []
    ai_data_list = [] 
    
    for conversation in data:
        mapping = conversation.get('mapping', {})
        create_time = conversation.get('create_time')
        base_dt = datetime.fromtimestamp(create_time) if create_time else None
        
        for node_id, node_data in mapping.items():
            message = node_data.get('message')
            if message and message.get('content') and message.get('author'):
                role = message['author']['role']
                msg_time = message.get('create_time')
                dt = datetime.fromtimestamp(msg_time) if msg_time else base_dt
                
                content_parts = message['content'].get('parts', [])
                text_content = "".join([part for part in content_parts if isinstance(part, str)])
                
                if text_content and dt:
                    item = {"text": text_content, "time": dt}
                    if role == 'user': user_data_list.append(item)
                    elif role == 'assistant': ai_data_list.append(item)
    return user_data_list, ai_data_list

# ==========================================
# 3. ç»Ÿè®¡å‡½æ•°
# ==========================================
def calculate_stats(data_list):
    if not data_list: return 0, 0, 0
    lengths = [len(d['text']) for d in data_list]
    total_len = sum(lengths)
    avg_len = total_len / len(lengths)
    max_len = max(lengths)
    return len(data_list), int(avg_len), max_len

# ==========================================
# 4. Emoji å½¢çŠ¶ç”Ÿæˆå™¨
# ==========================================
@st.cache_data
def create_emoji_mask(emoji_char):
    if emoji_char.startswith("â¬›"): return None
    char = emoji_char.split(" ")[0]
    img_size = (1000, 1000)
    img = Image.new("RGBA", img_size, (255,255,255,0)) 
    draw = ImageDraw.Draw(img)
    font_path = "seguiemj.ttf"
    try:
        font = ImageFont.truetype(font_path, 800)
    except:
        try: font = ImageFont.truetype("C:/Windows/Fonts/seguiemj.ttf", 800)
        except: return None
    try:
        bbox = draw.textbbox((0, 0), char, font=font)
        w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]
    except: w, h = 800, 800
    x, y = (img_size[0] - w) / 2, (img_size[1] - h) / 2 - 50
    draw.text((x, y), char, font=font, embedded_color=True) 
    alpha = img.split()[-1]
    alpha_array = np.array(alpha)
    final_mask = np.full(alpha_array.shape, 255, dtype=np.uint8)
    final_mask[alpha_array > 0] = 0
    return final_mask

# ==========================================
# 5. é¢œè‰²æˆªæ–­å™¨
# ==========================================
def get_truncated_cmap(cmap_name, min_val=0.0, max_val=1.0, n=256):
    cmap = plt.get_cmap(cmap_name)
    new_cmap = mcolors.LinearSegmentedColormap.from_list(
        f'trunc({cmap_name},{min_val:.2f},{max_val:.2f})',
        cmap(np.linspace(min_val, max_val, n))
    )
    return new_cmap

# ==========================================
# 6. ç•Œé¢ä¾§è¾¹æ 
# ==========================================
emoji_options = ["â¬› çŸ©å½¢ (æ— å½¢çŠ¶)", "ğŸ§  å¤§è„‘", "ğŸ—¨ æ°”æ³¡", "â˜ äº‘æœµ", "â¤ çˆ±å¿ƒ", "ğŸ¦Š ç‹ç‹¸", "ğŸŸ å°é±¼", "ğŸ± çŒ«å’ª", "ğŸ¦‰ çŒ«å¤´é¹°", "ğŸ¶ å°ç‹—"]
wordcloud_colormaps = {"Blues (è“)": "Blues", "Oranges (æ©™)": "Oranges", "Reds (çº¢)": "Reds", "Greens (ç»¿)": "Greens", "Purples (ç´«)": "Purples", "viridis (æå…‰)": "viridis", "magma (å²©æµ†)": "magma", "cool (å†·è‰²)": "cool", "autumn (ç§‹è‰²)": "autumn"}
USER_ICON = "ğŸ‘¾" 
AI_ICON = "ğŸ¦¾"

with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®é¢æ¿ v19.0")
    uploaded_file = st.file_uploader("1. ä¸Šä¼  conversations.json", type=['json'])
    
    st.markdown("---")
    st.header("ğŸ¨ è§†è§‰å®šåˆ¶")
    
    st.write("ğŸŒˆ **é¢œè‰²æµ“åº¦**")
    st.caption("è°ƒèŠ‚æ»‘å—ï¼Œè®©é¢œè‰²æ›´æ·±æˆ–æ›´æµ… ğŸ‘‡")
    color_intensity = st.slider("å»é™¤æµ…è‰²æ¯”ä¾‹", 0.0, 0.6, 0.3)

    st.markdown("---")
    c1, c2 = st.columns(2)
    with c1:
        st.markdown(f"<h4 style='text-align: center;'>{USER_ICON} ä½ </h4>", unsafe_allow_html=True)
        user_shape = st.selectbox("ä½ çš„å½¢çŠ¶", emoji_options, index=1)
        user_wc_color = st.selectbox("ä½ çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=0)
        
    with c2:
        st.markdown(f"<h4 style='text-align: center;'>{AI_ICON} AI</h4>", unsafe_allow_html=True)
        ai_shape = st.selectbox("AI çš„å½¢çŠ¶", emoji_options, index=2)
        ai_wc_color = st.selectbox("AI çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=1)

    st.markdown("---")
    max_words_limit = st.slider("ğŸ“Š æ•°æ®æ˜¾ç¤ºæ•°é‡", 20, 500, 100)
    
    st.markdown("---")
    st.header("ğŸ›¡ï¸ è¿‡æ»¤è®¾ç½®")
    use_default = st.toggle("ä½¿ç”¨å†…ç½®å‡€åŒ–è¯è¡¨", value=True)
    custom_input = st.text_area("è‡ªå®šä¹‰å±è”½è¯", height=80)
    final_stopwords = set(DEFAULT_STOPWORDS) if use_default else set()
    if custom_input: final_stopwords.update([w.strip().lower() for w in re.split(r'[ ,ï¼Œ\n]+', custom_input) if w.strip()])

# ==========================================
# 7. è¯äº‘é¢æ¿
# ==========================================
def show_wordcloud_panel(data_list, cmap_name, shape_mask, title, icon, limit, min_val):
    if not data_list: return
    text_list = [d['text'] for d in data_list]
    full_text = " ".join(text_list)
    words = jieba.lcut(full_text)
    filtered_words = [w.strip() for w in words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    word_counts = Counter(filtered_words)
    
    base_cmap_name = wordcloud_colormaps[cmap_name]
    custom_cmap = get_truncated_cmap(base_cmap_name, min_val=min_val, max_val=1.0)

    try:
        wc = WordCloud(font_path='msyh.ttc', width=1000, height=1000, background_color='white', colormap=custom_cmap, max_words=limit, stopwords=final_stopwords, mask=shape_mask, contour_width=0).generate_from_frequencies(word_counts)
        fig, ax = plt.subplots(figsize=(10, 10))
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e: st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    with st.expander(f"ğŸ“‹ æŸ¥çœ‹ {icon} {title} é«˜é¢‘è¯è¡¨", expanded=False):
        st.dataframe(pd.DataFrame(word_counts.most_common(limit), columns=['è¯è¯­', 'æ¬¡æ•°']), use_container_width=True, height=300)

# ==========================================
# 8. æŸ±çŠ¶å›¾é¢æ¿ (æ°´å¹³æ¸å˜ + å¤§é—´è·)
# ==========================================
def show_barchart_panel(data_list, cmap_name, role_icon, limit):
    if not data_list: return
    text_list = [d['text'] for d in data_list]
    words = [w.strip() for w in jieba.lcut(" ".join(text_list)) if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    common_words = Counter(words).most_common(limit)
    if not common_words: return
    
    df = pd.DataFrame(common_words, columns=['Word', 'Count']).sort_values(by='Count', ascending=True)
    
    # ã€å¢åŠ å‘¼å¸æ„Ÿã€‘æ¯è¡Œé«˜åº¦é¢„ç®—å¢åŠ åˆ° 0.45 (ä¹‹å‰æ˜¯0.25)
    height_per_row = 0.45
    dynamic_height = max(6, len(df) * height_per_row)
    
    fig, ax = plt.subplots(figsize=(12, dynamic_height))
    
    # è·å–æ¸å˜ colormap
    base_cmap_name = wordcloud_colormaps[cmap_name]
    # æˆªæ–­ä¸€ä¸‹ï¼Œä¿è¯å·¦è¾¹çš„æµ…è‰²ä¹Ÿèƒ½çœ‹è§ (ä» 0.3 å¼€å§‹)
    cmap = get_truncated_cmap(base_cmap_name, 0.3, 0.9)
    
    # ã€æ ¸å¿ƒé»‘ç§‘æŠ€ã€‘ç»˜åˆ¶å•æŸ±æ°´å¹³æ¸å˜
    # éå†æ¯ä¸€è¡Œæ•°æ®
    for i, (word, count) in enumerate(zip(df['Word'], df['Count'])):
        # åˆ›å»ºæ¸å˜çº¹ç† (1è¡Œ 256åˆ—)
        gradient = np.linspace(0, 1, 256).reshape(1, -1)
        # ä½¿ç”¨ imshow ç»˜åˆ¶å›¾ç‰‡ä½œä¸ºæŸ±å­
        # extent = [x_min, x_max, y_min, y_max]
        # yè½´ä½ç½®æ˜¯ iï¼Œæˆ‘ä»¬ç»™å®ƒ 0.6 çš„åšåº¦ (ä¸Šä¸‹å„0.3)
        ax.imshow(gradient, aspect='auto', cmap=cmap, extent=(0, count, i - 0.3, i + 0.3))
        
        # æ·»åŠ æ•°å­—æ ‡ç­¾ (ä½ç½®åœ¨ count å³ä¾§ä¸€ç‚¹ç‚¹)
        ax.text(count + (df['Count'].max() * 0.01), i, str(count), va='center', fontsize=12)

    # è®¾ç½® Y è½´æ ‡ç­¾ (è¯è¯­)
    ax.set_yticks(range(len(df)))
    ax.set_yticklabels(df['Word'], fontsize=14)
    
    # ç¾åŒ–å›¾è¡¨
    ax.set_title(f"{role_icon} Top {limit} è¯é¢‘ç»Ÿè®¡", fontsize=20, pad=20)
    ax.set_ylim(-0.5, len(df) - 0.5) # ç¡®ä¿ä¸Šä¸‹ç•™ç™½åˆé€‚
    ax.set_xlim(0, df['Count'].max() * 1.15) # Xè½´ç•™å‡ºç©ºé—´ç»™æ•°å­—
    
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False) # å»æ‰å·¦è¾¹æ¡†ï¼Œæ›´ç°ä»£
    ax.spines['bottom'].set_visible(False) # å»æ‰åº•è¾¹æ¡†
    
    # åªä¿ç•™æ¨ªå‘è™šçº¿ï¼Œè¾…åŠ©é˜…è¯»
    ax.grid(axis='x', linestyle='--', alpha=0.2)
    ax.tick_params(axis='x', labelsize=10)
    
    plt.tight_layout()
    st.pyplot(fig)

# ==========================================
# 9. æ—¶å…‰æœº
# ==========================================
def show_timeline_panel(user_list):
    st.markdown("### ğŸ“… æœˆåº¦è¯é¢˜æ—¶å…‰æœº (æ·±åº¦å»å™ª)")
    st.caption("å·²è‡ªåŠ¨å‰”é™¤å…¨å±€æœ€å¸¸ç”¨çš„ 50 ä¸ªè¯ï¼Œåªæ˜¾ç¤ºæ¯æœˆçš„ç‹¬ç‰¹è¯é¢˜ã€‚")
    
    df_u = pd.DataFrame(user_list)
    if df_u.empty: 
        st.warning("æ²¡æœ‰è§£æåˆ°æ—¶é—´æ•°æ®ã€‚")
        return
    
    all_text = " ".join(df_u['text'].tolist())
    all_words = jieba.lcut(all_text)
    all_filtered = [w.strip() for w in all_words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    global_counter = Counter(all_filtered)
    global_noise_words = set([w for w, c in global_counter.most_common(50)])
    
    df_u['month'] = df_u['time'].dt.to_period('M')
    monthly_groups = df_u.groupby('month')
    
    timeline_data = []
    for month, group in monthly_groups:
        month_text = " ".join(group['text'].tolist())
        words = jieba.lcut(month_text)
        filtered = [w.strip() for w in words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords and w.strip() not in global_noise_words]
        top_n = Counter(filtered).most_common(10) 
        top_str = " | ".join([f"{w}" for w, c in top_n])
        timeline_data.append({"æœˆä»½": str(month), "æœ¬æœˆç‰¹è‰²è¯é¢˜ (Top 10)": top_str, "å¯¹è¯æ¡æ•°": len(group)})
    
    df_timeline = pd.DataFrame(timeline_data).sort_values(by="æœˆä»½", ascending=False)
    st.dataframe(df_timeline, use_container_width=True, height=600)

# ==========================================
# ä¸»ç•Œé¢
# ==========================================
st.title("ğŸ›¸ ChatGPT æ·±åº¦åˆ†æ 19.0")

if uploaded_file:
    user_mask = create_emoji_mask(user_shape)
    ai_mask = create_emoji_mask(ai_shape)
    user_data, ai_data = parse_data(uploaded_file)
    
    u_count, u_avg, u_max = calculate_stats(user_data)
    a_count, a_avg, a_max = calculate_stats(ai_data)
    st.markdown("### ğŸ§¬ èŠå¤©åŸºå› æŠ¥å‘Š")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric(f"{USER_ICON} ä½ çš„æ€»å‘è¨€", f"{u_count} æ¡")
    col2.metric(f"{USER_ICON} ä½ çš„å¹³å‡é•¿åº¦", f"{u_avg} å­—/æ¡")
    col3.metric(f"{AI_ICON} AI çš„æ€»å›å¤", f"{a_count} æ¡")
    col4.metric(f"{AI_ICON} AI çš„å¹³å‡é•¿åº¦", f"{a_avg} å­—/æ¡", delta=f"{a_avg - u_avg} (è¡¨è¾¾æ¬²)" if a_avg > u_avg else None)
    st.markdown("---")

    tab1, tab2, tab3 = st.tabs(["ğŸ¨ è¯äº‘ & è¯è¡¨", "ğŸ“Š å®åŠ›å¯¹æ¯”", "ğŸ“… æ—¶å…‰æœº"])
    
    with tab1:
        c1, c2 = st.columns(2)
        with c1: st.subheader(f"{USER_ICON} ä½ çš„è¯äº‘"); show_wordcloud_panel(user_data, user_wc_color, user_mask, "ç”¨æˆ·", USER_ICON, max_words_limit, color_intensity)
        with c2: st.subheader(f"{AI_ICON} AI çš„è¯äº‘"); show_wordcloud_panel(ai_data, ai_wc_color, ai_mask, "AI", AI_ICON, max_words_limit, color_intensity)
    with tab2:
        c1, c2 = st.columns(2)
        with c1: show_barchart_panel(user_data, user_wc_color, USER_ICON, max_words_limit)
        with c2: show_barchart_panel(ai_data, ai_wc_color, AI_ICON, max_words_limit)
    with tab3: 
        show_timeline_panel(user_data)
else: st.write("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶å¼€å§‹")