import streamlit as st
import json
import pandas as pd
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.font_manager as fm
from collections import Counter
import re
import numpy as np
from PIL import Image, ImageFont, ImageDraw
from datetime import datetime
import os

# ==========================================
# 0. åŸºç¡€é…ç½® & CSS (å¼ºåŠ›å±…ä¸­ä¿®å¤ç‰ˆ)
# ==========================================
st.set_page_config(page_title="ChatGPT æ·±åº¦åˆ†æ 25.0", layout="wide", page_icon="ğŸ“Š")

st.markdown("""
<style>
/* 1. ä¾§è¾¹æ å…¨å±€æ–‡æœ¬å±…ä¸­ */
section[data-testid="stSidebar"] .stMarkdown h1,
section[data-testid="stSidebar"] .stMarkdown h2,
section[data-testid="stSidebar"] .stMarkdown h3,
section[data-testid="stSidebar"] .stMarkdown p,
section[data-testid="stSidebar"] .stMarkdown h4,
section[data-testid="stSidebar"] label, 
section[data-testid="stSidebar"] .stCaption,
section[data-testid="stSidebar"] div[data-testid="stText"] {
    text-align: center !important;
    width: 100% !important;
    display: block !important;
}

/* 2. é¢œè‰²é€‰æ‹©å™¨å±…ä¸­ */
div[data-testid="stColorPicker"] {
    display: flex;
    justify-content: center;
    align-items: center;
    flex-direction: column;
    width: 100%;
}

/* 3. ä¿®å¤ Toggle å¼€å…³å’Œ Text Area æ ‡é¢˜çš„å±…ä¸­ */
div[data-testid="stCheckbox"] {
    justify-content: center;
}
div[data-testid="stTextArea"] label {
    text-align: center !important;
}

/* 4. è°ƒæ•´ Sidebar é¡¶éƒ¨è¾¹è· */
section[data-testid="stSidebar"] > div:first-child {
    padding-top: 2rem;
}
</style>
""", unsafe_allow_html=True)

# ==========================================
# 1. å­—ä½“åŠ è½½å™¨
# ==========================================
def get_custom_font_path():
    font_path = "msyh.ttc"
    if os.path.exists(font_path):
        return font_path
    else:
        return "arial.ttf" 

def get_custom_font_prop(size=14, weight='normal'):
    fp = get_custom_font_path()
    # æ˜¾å¼è®¾ç½®å¤§å°å’Œç²—ç»†
    prop = fm.FontProperties(fname=fp)
    prop.set_size(size)
    prop.set_weight(weight)
    return prop

# ==========================================
# 2. å†…ç½®åœç”¨è¯è¡¨
# ==========================================
DEFAULT_STOPWORDS = {
    "---", "...", "###", "___", "***", "=>", "->", "<-", "<=", ">=", "**", "__", "~~", "==", "!=", "&&", "||", "```", "`", ">", "|", "[", "]", "{", "}", "(", ")",
    "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½", "è‡ªå·±", "è¿™", "é‚£", "å—", "å§", "å•Š", "å¯ä»¥", "è¿™ä¸ª", "é‚£ä¸ª", "å…¶å®", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "ä½†æ˜¯", 
    "å—¯", "å“¦", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸º", "ä¹‹", "ä¸", "åŠ", "å…¶", "ä»¥", "å¯¹", "è€Œ", "ç€", "ä¸‹", "åœ°", "å­", "ä¸­", "å¥¹", "ä»–", "ä»¬", "æˆ‘ä»¬", "ä½ ä»¬", "ä»–ä»¬", "å®ƒä»¬", 
    "å“ˆå“ˆ", "å“ˆå“ˆå“ˆ", "å˜¿å˜¿", "å‘ƒ", "å˜›", "å‘€", "å‘¢", "å•¦", "å“‡", "å”‰", "å“", "å“¼", "å™¢", "å‘—",
    "å“ªæ€•", "å¯èƒ½", "è§‰å¾—", "åº”è¯¥", "è¿™ç§", "é‚£ç§", "æ¯”å¦‚", "æˆ–è€…", "ä»¥åŠ", "ç„¶å", "è™½ç„¶", "ä½†æ˜¯", "ä¸è¿‡", "åªæ˜¯", "è¿™æ ·", "é‚£æ ·", "è¿™é‡Œ", "é‚£é‡Œ",
    "ç¡®å®", "çœŸçš„", "éå¸¸", "ç‰¹åˆ«", "æ¯”è¾ƒ", "ç¨å¾®", "åªè¦", "åªæœ‰", "é™¤é", "å‡ ä¹", "ç®€ç›´", "ä¼¼ä¹", "å¥½åƒ", "å¤§æ¦‚", "ä¹Ÿè®¸",
    "1", "2", "3", "4", "5", "6", "7", "8", "9", "0", ".", "ã€", "ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", "â€œ", "â€", "ï¼š", "ï¼›", "ï¼ˆ", "ï¼‰", "ã€Š", "ã€‹", "â€¦", "â€”", "ï½",
    "the", "a", "an", "and", "or", "but", "if", "because", "as", "what", "when", "where", "how", "why", "which", "who", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do", "does", "did", "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "my", "your", "his", "its", "our", "their", "this", "that", "these", "those", "there", "here", "in", "on", "at", "to", "for", "of", "with", "by", "from", "up", "down", "out", "about", "into", "over", "after", "can", "could", "will", "would", "should", "may", "might", "must", "not", "no", "yes", "ok", "okay", "well", "just", "only", "very", "really", "too", "so", "much", "many", "some", "any", "all"
}

# ==========================================
# 3. æ ¸å¿ƒè§£æå‡½æ•°
# ==========================================
@st.cache_data
def parse_data(file):
    try:
        data = json.load(file)
    except:
        st.error("æ–‡ä»¶æ ¼å¼ä¸å¯¹ï¼Œè¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯ JSON æ–‡ä»¶")
        return [], [], []
    
    user_data_list = []
    ai_data_list = [] 
    
    for conversation in data:
        mapping = conversation.get('mapping', {})
        create_time = conversation.get('create_time')
        base_dt = datetime.fromtimestamp(create_time) if create_time else None
        
        for node_id, node_data in mapping.items():
            message = node_data.get('message')
            if message and message.get('content') and message.get('author'):
                role = message['author']['role']
                msg_time = message.get('create_time')
                dt = datetime.fromtimestamp(msg_time) if msg_time else base_dt
                content_parts = message['content'].get('parts', [])
                text_content = "".join([part for part in content_parts if isinstance(part, str)])
                
                if text_content and dt:
                    item = {"text": text_content, "time": dt}
                    if role == 'user': user_data_list.append(item)
                    elif role == 'assistant': ai_data_list.append(item)
    return user_data_list, ai_data_list

# ==========================================
# 4. ç»Ÿè®¡å‡½æ•°
# ==========================================
def calculate_stats(data_list):
    if not data_list: return 0, 0, 0
    lengths = [len(d['text']) for d in data_list]
    total_len = sum(lengths)
    avg_len = total_len / len(lengths)
    max_len = max(lengths)
    return len(data_list), int(avg_len), max_len

# ==========================================
# 5. é¢œè‰²æˆªæ–­å™¨
# ==========================================
def get_truncated_cmap(cmap_name, min_val=0.0, max_val=1.0, n=256):
    cmap = plt.get_cmap(cmap_name)
    new_cmap = mcolors.LinearSegmentedColormap.from_list(
        f'trunc({cmap_name},{min_val:.2f},{max_val:.2f})',
        cmap(np.linspace(min_val, max_val, n))
    )
    return new_cmap

# ==========================================
# 6. ç•Œé¢ä¾§è¾¹æ 
# ==========================================
wordcloud_colormaps = {"Blues (è“)": "Blues", "Oranges (æ©™)": "Oranges", "Reds (çº¢)": "Reds", "Greens (ç»¿)": "Greens", "Purples (ç´«)": "Purples", "viridis (æå…‰)": "viridis", "magma (å²©æµ†)": "magma", "cool (å†·è‰²)": "cool", "autumn (ç§‹è‰²)": "autumn"}
USER_ICON = "ğŸ‘¾" 
AI_ICON = "ğŸ¦¾"

with st.sidebar:
    st.markdown("<h1>âš™ï¸ è®¾ç½®é¢æ¿ v25.0</h1>", unsafe_allow_html=True)
    uploaded_file = st.file_uploader("1. ä¸Šä¼  conversations.json", type=['json'])
    
    st.markdown("---")
    st.header("ğŸ¨ è§†è§‰å®šåˆ¶")
    
    st.write("ğŸŒˆ **é¢œè‰²æµ“åº¦**")
    color_intensity = st.slider("å»é™¤æµ…è‰²æ¯”ä¾‹", 0.0, 0.6, 0.3)

    st.markdown("---")
    c1, c2 = st.columns(2)
    with c1:
        st.markdown(f"<h4>{USER_ICON} ä½ </h4>", unsafe_allow_html=True)
        user_wc_color = st.selectbox("ä½ çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=0)
        
    with c2:
        st.markdown(f"<h4>{AI_ICON} AI</h4>", unsafe_allow_html=True)
        ai_wc_color = st.selectbox("AI çš„è‰²ç³»", list(wordcloud_colormaps.keys()), index=1)

    st.markdown("---")
    max_words_limit = st.slider("ğŸ“Š æ•°æ®æ˜¾ç¤ºæ•°é‡", 20, 500, 100)
    
    st.markdown("---")
    st.header("ğŸ›¡ï¸ è¿‡æ»¤è®¾ç½®")
    use_default = st.toggle("ä½¿ç”¨å†…ç½®å‡€åŒ–è¯è¡¨", value=True)
    custom_input = st.text_area("è‡ªå®šä¹‰å±è”½è¯", height=80)
    final_stopwords = set(DEFAULT_STOPWORDS) if use_default else set()
    if custom_input: final_stopwords.update([w.strip().lower() for w in re.split(r'[ ,ï¼Œ\n]+', custom_input) if w.strip()])

# ==========================================
# 7. è¯äº‘é¢æ¿ (ä¿®å¤ï¼šä½¿ç”¨çº¯ç™½æ–¹å—Mask)
# ==========================================
def show_wordcloud_panel(data_list, cmap_name, title, icon, limit, min_val):
    if not data_list: return
    text_list = [d['text'] for d in data_list]
    full_text = " ".join(text_list)
    words = jieba.lcut(full_text)
    filtered_words = [w.strip() for w in words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    word_counts = Counter(filtered_words)
    
    base_cmap_name = wordcloud_colormaps[cmap_name]
    custom_cmap = get_truncated_cmap(base_cmap_name, min_val=min_val, max_val=1.0)
    fp = get_custom_font_path()

    # ã€ä¿®å¤ã€‘åˆ›å»ºä¸€ä¸ª 1000x1000 çš„çº¯ç™½ maskï¼Œå¼ºåˆ¶è¯äº‘åœ¨è¿™ä¸ªæ­£æ–¹å½¢å†…
    # è¿™æ ·å¯ä»¥ä¿è¯å®ƒåƒä»¥å‰ä¸€æ ·é¥±æ»¡ï¼Œè€Œä¸æ˜¯å˜æˆæ‰é•¿çš„çŸ©å½¢
    square_mask = np.array(Image.new("RGB", (1000, 1000), (255, 255, 255)))

    try:
        wc = WordCloud(
            font_path=fp, 
            width=1000, height=1000, # å¼ºåˆ¶æ­£æ–¹å½¢
            background_color='white', 
            colormap=custom_cmap, 
            max_words=limit, 
            stopwords=final_stopwords,
            mask=square_mask, # ä½¿ç”¨æ­£æ–¹å½¢ Mask
            contour_width=0
        ).generate_from_frequencies(word_counts)
        
        fig, ax = plt.subplots(figsize=(10, 10)) # ç”»å¸ƒä¹Ÿæ˜¯æ­£æ–¹å½¢
        ax.imshow(wc, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
    except Exception as e: st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
    
    with st.expander(f"ğŸ“‹ æŸ¥çœ‹ {icon} {title} é«˜é¢‘è¯è¡¨", expanded=False):
        st.dataframe(pd.DataFrame(word_counts.most_common(limit), columns=['è¯è¯­', 'æ¬¡æ•°']), use_container_width=True, height=300)

# ==========================================
# 8. æŸ±çŠ¶å›¾é¢æ¿ (ä¿®å¤ï¼šå¤§å·ç²—ä½“æ ‡é¢˜)
# ==========================================
def show_barchart_panel(data_list, cmap_name, plain_text_title, limit):
    if not data_list: return
    text_list = [d['text'] for d in data_list]
    words = [w.strip() for w in jieba.lcut(" ".join(text_list)) if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    common_words = Counter(words).most_common(limit)
    if not common_words: return
    
    df = pd.DataFrame(common_words, columns=['Word', 'Count']).sort_values(by='Count', ascending=True)
    height_per_row = 0.45
    dynamic_height = max(6, len(df) * height_per_row)
    fig, ax = plt.subplots(figsize=(12, dynamic_height))
    
    # ã€ä¿®å¤ã€‘ç”Ÿæˆä¸¤ç§å­—ä½“å±æ€§ï¼šä¸€ç§æ™®é€šï¼ˆç»™åæ ‡è½´ï¼‰ï¼Œä¸€ç§è¶…å¤§åŠ ç²—ï¼ˆç»™æ ‡é¢˜ï¼‰
    font_normal = get_custom_font_prop(size=14)
    font_title = get_custom_font_prop(size=50, weight='bold') # 50å·ç²—ä½“ï¼
    
    base_cmap_name = wordcloud_colormaps[cmap_name]
    cmap = get_truncated_cmap(base_cmap_name, 0.3, 0.9)
    
    for i, (word, count) in enumerate(zip(df['Word'], df['Count'])):
        gradient = np.linspace(0, 1, 256).reshape(1, -1)
        ax.imshow(gradient, aspect='auto', cmap=cmap, extent=(0, count, i - 0.3, i + 0.3))
        ax.text(count + (df['Count'].max() * 0.01), i, str(count), va='center', fontsize=12)

    ax.set_yticks(range(len(df)))
    ax.set_yticklabels(df['Word'], fontproperties=font_normal)
    
    # ã€ä¿®å¤ã€‘åº”ç”¨è¶…å¤§ç²—ä½“æ ‡é¢˜
    ax.set_title(f"{plain_text_title} Top {limit} è¯é¢‘ç»Ÿè®¡", pad=40, fontproperties=font_title)
    
    ax.set_ylim(-0.5, len(df) - 0.5) 
    ax.set_xlim(0, df['Count'].max() * 1.15) 
    
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.grid(axis='x', linestyle='--', alpha=0.2)
    ax.tick_params(axis='x', labelsize=10)
    
    plt.tight_layout()
    st.pyplot(fig)

# ==========================================
# 9. æ—¶å…‰æœº
# ==========================================
def show_timeline_panel(user_list):
    st.markdown("### ğŸ“… æœˆåº¦è¯é¢˜æ—¶å…‰æœº (æ·±åº¦å»å™ª)")
    st.caption("å·²è‡ªåŠ¨å‰”é™¤å…¨å±€æœ€å¸¸ç”¨çš„ 50 ä¸ªè¯ï¼Œåªæ˜¾ç¤ºæ¯æœˆçš„ç‹¬ç‰¹è¯é¢˜ã€‚")
    
    df_u = pd.DataFrame(user_list)
    if df_u.empty: 
        st.warning("æ²¡æœ‰è§£æåˆ°æ—¶é—´æ•°æ®ã€‚")
        return
    
    all_text = " ".join(df_u['text'].tolist())
    all_words = jieba.lcut(all_text)
    all_filtered = [w.strip() for w in all_words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords]
    global_counter = Counter(all_filtered)
    global_noise_words = set([w for w, c in global_counter.most_common(50)])
    
    df_u['month'] = df_u['time'].dt.to_period('M')
    monthly_groups = df_u.groupby('month')
    
    timeline_data = []
    for month, group in monthly_groups:
        month_text = " ".join(group['text'].tolist())
        words = jieba.lcut(month_text)
        filtered = [w.strip() for w in words if len(w.strip()) > 1 and w.strip().lower() not in final_stopwords and w.strip() not in global_noise_words]
        top_n = Counter(filtered).most_common(10) 
        top_str = " | ".join([f"{w}" for w, c in top_n])
        timeline_data.append({"æœˆä»½": str(month), "æœ¬æœˆç‰¹è‰²è¯é¢˜ (Top 10)": top_str, "å¯¹è¯æ¡æ•°": len(group)})
    
    df_timeline = pd.DataFrame(timeline_data).sort_values(by="æœˆä»½", ascending=False)
    st.dataframe(df_timeline, use_container_width=True, height=600)

# ==========================================
# ä¸»ç•Œé¢
# ==========================================
st.title("ğŸ›¸ ChatGPT æ·±åº¦åˆ†æ 25.0")

if uploaded_file:
    user_data, ai_data = parse_data(uploaded_file)
    
    u_count, u_avg, u_max = calculate_stats(user_data)
    a_count, a_avg, a_max = calculate_stats(ai_data)
    st.markdown("### ğŸ§¬ èŠå¤©åŸºå› æŠ¥å‘Š")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric(f"{USER_ICON} ä½ çš„æ€»å‘è¨€", f"{u_count} æ¡")
    col2.metric(f"{USER_ICON} ä½ çš„å¹³å‡é•¿åº¦", f"{u_avg} å­—/æ¡")
    col3.metric(f"{AI_ICON} AI çš„æ€»å›å¤", f"{a_count} æ¡")
    col4.metric(f"{AI_ICON} AI çš„å¹³å‡é•¿åº¦", f"{a_avg} å­—/æ¡", delta=f"{a_avg - u_avg} (è¡¨è¾¾æ¬²)" if a_avg > u_avg else None)
    st.markdown("---")

    tab1, tab2, tab3 = st.tabs(["ğŸ¨ è¯äº‘ & è¯è¡¨", "ğŸ“Š å®åŠ›å¯¹æ¯”", "ğŸ“… æ—¶å…‰æœº"])
    
    with tab1:
        c1, c2 = st.columns(2)
        with c1: st.subheader(f"{USER_ICON} ä½ çš„è¯äº‘"); show_wordcloud_panel(user_data, user_wc_color, "ç”¨æˆ·", USER_ICON, max_words_limit, color_intensity)
        with c2: st.subheader(f"{AI_ICON} AI çš„è¯äº‘"); show_wordcloud_panel(ai_data, ai_wc_color, "AI", AI_ICON, max_words_limit, color_intensity)
    with tab2:
        c1, c2 = st.columns(2)
        with c1: show_barchart_panel(user_data, user_wc_color, "ç”¨æˆ·", max_words_limit)
        with c2: show_barchart_panel(ai_data, ai_wc_color, "AI", max_words_limit)
    with tab3: 
        show_timeline_panel(user_data)
else: st.write("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶å¼€å§‹")